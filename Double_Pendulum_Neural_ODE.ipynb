{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dR6Gg5Y7pYTr",
        "outputId": "3e80e817-87a7-4dc1-d34c-316c26dc64f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->torchdiffeq) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.3)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTpR8_Qw5mbe"
      },
      "outputs": [],
      "source": [
        "# 1 Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchdiffeq import odeint\n",
        "\n",
        "## Synthetic part\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAP5yoD3Z9_U"
      },
      "outputs": [],
      "source": [
        "#2 Physical Model Params\n",
        "\n",
        "# Constants\n",
        "g = 9.81\n",
        "L1, L2 = 1.0, 1.0  # lengths of the pendulums\n",
        "m1, m2 = 1.0, 1.0  # masses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "DkUoDWLLaJmR",
        "outputId": "3d3136aa-8472-43a1-8e97-c590d1859931"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#3.1 Generating Y actual - for training and testing data\n",
        "\n",
        "# Differential equations for the double pendulum\n",
        "def double_pendulum(t, y):\n",
        "    θ1, ω1, θ2, ω2 = y\n",
        "    Δ = θ2 - θ1\n",
        "\n",
        "    den1 = (2 * m1 + m2 - m2 * np.cos(2 * Δ))\n",
        "    den2 = den1\n",
        "\n",
        "    dθ1_dt = ω1\n",
        "\n",
        "    dω1_dt = (\n",
        "        -g * (2 * m1 + m2) * np.sin(θ1)\n",
        "        - m2 * g * np.sin(θ1 - 2 * θ2)\n",
        "        - 2 * np.sin(Δ) * m2 * (ω2**2 * L2 + ω1**2 * L1 * np.cos(Δ))\n",
        "    ) / (L1 * den1)\n",
        "\n",
        "    dθ2_dt = ω2\n",
        "\n",
        "    dω2_dt = (\n",
        "        2 * np.sin(Δ) * (\n",
        "            ω1**2 * L1 * (m1 + m2)\n",
        "            + g * (m1 + m2) * np.cos(θ1)\n",
        "            + ω2**2 * L2 * m2 * np.cos(Δ)\n",
        "        )\n",
        "    ) / (L2 * den2)\n",
        "\n",
        "    return [dθ1_dt, dω1_dt, dθ2_dt, dω2_dt]\n",
        "\n",
        "# Initial conditions: [theta1, omega1, theta2, omega2]\n",
        "y0_np = [np.pi / 2, 0, np.pi / 2, 0]\n",
        "\n",
        "# Time grid\n",
        "t_span = (0, 20)\n",
        "t_eval = np.linspace(*t_span, 1000)\n",
        "\n",
        "# Solve the ODE\n",
        "sol = solve_ivp(double_pendulum, t_span, y0_np, t_eval=t_eval)\n",
        "\n",
        "# Extract angles\n",
        "theta1 = sol.y[0]\n",
        "theta2 = sol.y[2]\n",
        "\n",
        "Y_true  = sol.y.T.astype(np.float32)   # shape (T, 4)\n",
        "\n",
        "\n",
        "# Convert to Cartesian coordinates\n",
        "x1 = L1 * np.sin(theta1)\n",
        "y1 = -L1 * np.cos(theta1)\n",
        "\n",
        "x2 = x1 + L2 * np.sin(theta2)\n",
        "y2 = y1 - L2 * np.cos(theta2)\n",
        "\n",
        "# Animation setup\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(-2.2, 2.2)\n",
        "ax.set_ylim(-2.2, 2.2)\n",
        "ax.set_title(\"Double Pendulum Animation\")\n",
        "\n",
        "line, = ax.plot([], [], 'o-', lw=2)\n",
        "trace, = ax.plot([], [], '-', lw=1, color='orange', alpha=0.5)\n",
        "trace_x, trace_y = [], []\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    trace.set_data([], [])\n",
        "    return line, trace\n",
        "\n",
        "def update(i):\n",
        "    thisx = [0, x1[i], x2[i]]\n",
        "    thisy = [0, y1[i], y2[i]]\n",
        "\n",
        "    line.set_data(thisx, thisy)\n",
        "\n",
        "    trace_x.append(x2[i])\n",
        "    trace_y.append(y2[i])\n",
        "    trace.set_data(trace_x, trace_y)\n",
        "\n",
        "    return line, trace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# #3.2 Using the physical pendulum dataset\n",
        "# # Use the real double pendulum dataset: reconstruct [θ1, ω1, θ2, ω2]\n",
        "# trials = ['Trial1', 'Trial2', 'Trial3']\n",
        "\n",
        "# all_t = []\n",
        "# all_states = []   # each element will be (T, 4) = [θ1, ω1, θ2, ω2]\n",
        "\n",
        "# for trial in trials:\n",
        "#     # Load angle means\n",
        "#     DPmean_data_RB0 = np.load(os.path.join(trial, 'DPmean_data_RB0.npy'))\n",
        "#     DPmean_data_RB1 = np.load(os.path.join(trial, 'DPmean_data_RB1.npy'))\n",
        "\n",
        "#     # Assume shape (2, N): [0]=time, [1]=angle (radians)\n",
        "#     t_trial = DPmean_data_RB0[0]   # (N,)\n",
        "#     phi1    = DPmean_data_RB0[1]   # (N,)\n",
        "#     phi2    = DPmean_data_RB1[1]   # (N,)\n",
        "\n",
        "#     # Angular velocities via finite differences: dφ/dt (rad/s)\n",
        "#     omega1 = np.gradient(phi1, t_trial)\n",
        "#     omega2 = np.gradient(phi2, t_trial)\n",
        "\n",
        "#     # Stack into (N, 4) : [θ1, ω1, θ2, ω2]\n",
        "#     states_trial = np.stack([phi1, omega1, phi2, omega2], axis=-1)  # (N, 4)\n",
        "\n",
        "#     all_t.append(t_trial)\n",
        "#     all_states.append(states_trial.astype(np.float32))\n",
        "\n",
        "#     print(f\"{trial}: t shape = {t_trial.shape}, states shape = {states_trial.shape}\")\n",
        "\n",
        "# # For now, just train on Trial1\n",
        "# t_eval = all_t[0]          # (T,)\n",
        "# Y_true = all_states[0]     # (T, 4), dtype float32\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Using REAL pendulum data from\", trials[0])\n",
        "# print(\"t_eval shape:\", t_eval.shape)\n",
        "# print(\"Y_true shape:\", Y_true.shape)\n",
        "\n",
        "# DS = 40  # down sampling bc dataset is 38k points\n",
        "# t_eval = t_eval[::DS]\n",
        "# Y_true = Y_true[::DS]\n",
        "\n",
        "# print(\"After downsampling:\")\n",
        "# print(\"t_eval shape:\", t_eval.shape)\n",
        "# print(\"Y_true shape:\", Y_true.shape)"
      ],
      "metadata": {
        "id": "yVn8rjgGHMqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68eGREG_qAab"
      },
      "outputs": [],
      "source": [
        "#4 Torch Tensors setting up the ODE\n",
        "torch.manual_seed(67)  # Manual seed for randomization\n",
        "\n",
        "# time vector (T,)\n",
        "t = torch.tensor(t_eval, dtype=torch.float32)\n",
        "\n",
        "# initial state y0 = [θ1(0), ω1(0), θ2(0), ω2(0)]\n",
        "y0_torch = torch.tensor(Y_true[0], dtype=torch.float32)  # shape (4,)\n",
        "\n",
        "# target trajectory (T, 4)\n",
        "Y_true_t = torch.tensor(Y_true, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# ODE function f in dy/dt = f(t,y)\n",
        "class ODEFunc(nn.Module):\n",
        "  def __init__(self, in_dim = 4, out_dim=4, hidden = (10, 10, 10, 10),act=nn.Tanh):\n",
        "    # num_in_out = 4 for the 2 angles and 2 angular velocities (or whichever parameters you want)\n",
        "    # h1 is the number of neurons in the hidden layer, 128 is arbitrary for now\n",
        "    # if we decide to add more hidden layers, we can do h2, h3, etc.\n",
        "    super().__init__()\n",
        "    dims = [in_dim, *hidden, out_dim]\n",
        "\n",
        "    # The following is an MLP with x hidden layers\n",
        "    layers = []\n",
        "    for i in range(len(dims) - 1):\n",
        "      layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "      if i < len(dims) - 2:           # no activation after last Linear\n",
        "        layers.append(act())\n",
        "    self.net = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, t, y):\n",
        "    # takes (4,) or (B,4) and returns same shape\n",
        "\n",
        "    # y is a torch vector representing the current state\n",
        "    # t is there just to match the expected function signature\n",
        "    # need to make sure that the shapes are of form (B, 4)\n",
        "    squeezed = False\n",
        "    if y.dim() == 1:\n",
        "      y = y.unsqueeze(0)  # (1,4)\n",
        "      squeezed = True\n",
        "    dy = self.net(y)         # (B,4)\n",
        "    return dy.squeeze(0) if squeezed else dy # if y is (4,), Linear wants (B,4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYfAKlOBpRST"
      },
      "outputs": [],
      "source": [
        "#5 Neural ODE model\n",
        "\n",
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, odefunc, method='', rtol=1e-6, atol=1e-6):\n",
        "        super().__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.method = method\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "        self._odeint = odeint\n",
        "\n",
        "    def forward(self, y0, t):\n",
        "        return self._odeint(self.odefunc, y0, t,\n",
        "                            method=self.method, rtol=self.rtol, atol=self.atol)\n",
        "\n",
        "\n",
        "# import torch\n",
        "# from torchdiffeq import odeint\n",
        "\n",
        "# _FIXED_STEP = {\"euler\", \"midpoint\", \"rk4\", \"explicit_adams\", \"implicit_adams\"}\n",
        "\n",
        "# class NeuralODE(torch.nn.Module):\n",
        "#     def __init__(self, f, method=\"rk4\", step_size=None, rtol=1e-5, atol=1e-7):\n",
        "#         super().__init__()\n",
        "#         self.f = f\n",
        "#         self.method = method\n",
        "#         self.step_size = step_size\n",
        "#         self.rtol = rtol\n",
        "#         self.atol = atol\n",
        "\n",
        "#     def forward(self, y0, t):\n",
        "#         kwargs = {\"method\": self.method}\n",
        "\n",
        "#         if self.method in _FIXED_STEP:\n",
        "#             # fixed-step solvers require a step size\n",
        "#             step = float(t[1] - t[0]) if self.step_size is None else float(self.step_size)\n",
        "#             kwargs[\"options\"] = {\"step_size\": step}\n",
        "#         else:\n",
        "#             # adaptive solvers use tolerances\n",
        "#             kwargs[\"rtol\"] = self.rtol\n",
        "#             kwargs[\"atol\"] = self.atol\n",
        "\n",
        "#         # returns shape (T, state_dim)\n",
        "#         return odeint(self.f, y0, t, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_btqVRWRRkl"
      },
      "outputs": [],
      "source": [
        "# #6 Training setup\n",
        "\n",
        "# f = ODEFunc(in_dim = 4, out_dim=4, hidden=(10,10))\n",
        "# opt = torch.optim.Adam(f.parameters(), lr=1e-3)\n",
        "# neural_ode = NeuralODE(f, method='rk4', rtol=1e-6, atol=1e-6)\n",
        "# max_epochs = 200\n",
        "\n",
        "# t_train      = t\n",
        "# Y_true_train = Y_true_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo_8vFxmckEw"
      },
      "outputs": [],
      "source": [
        "#7 Training Loop\n",
        "\n",
        "f.train()\n",
        "for epoch in range(max_epochs):\n",
        "    # Integrate NN-defined dynamics\n",
        "    Y_hat = neural_ode(y0_torch, t)\n",
        "    loss  = F.mse_loss(Y_hat, Y_true_t)\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"epoch {epoch}: loss={loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #8 Training the Neural ODE\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def rmse(pred, target):\n",
        "#     return torch.sqrt(torch.mean((pred - target) ** 2)).item()\n",
        "\n",
        "# # Pick solvers to compare\n",
        "# methods_to_try = [\n",
        "#     (\"euler\",    dict(method=\"euler\")),\n",
        "#     (\"midpoint\", dict(method=\"midpoint\")),\n",
        "#     (\"rk4\",      dict(method=\"rk4\")),\n",
        "#     (\"dopri5\",   dict(method=\"dopri5\", rtol=1e-5, atol=1e-7)),\n",
        "#     (\"bosh3\",    dict(method=\"bosh3\",  rtol=1e-5, atol=1e-7)),\n",
        "# ]\n",
        "\n",
        "# num_epochs = 200\n",
        "# lr = 1e-3\n",
        "\n",
        "# results = {}\n",
        "\n",
        "# for name, solver_kwargs in methods_to_try:\n",
        "#     torch.manual_seed(67)\n",
        "\n",
        "#     # IMPORTANT: create a fresh dynamics net and NeuralODE each time\n",
        "#     func = ODEFunc()                 # <-- use YOUR class name from #5 (ODEFunc / ODEFuncNN etc.)\n",
        "#     model = NeuralODE(func, **solver_kwargs)\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#     criterion = nn.MSELoss()\n",
        "\n",
        "#     loss_hist = []\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         Y_hat = model(y0_torch, t)        # (T,4)\n",
        "#         loss = criterion(Y_hat, Y_true_t) # compare to data\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         loss_hist.append(loss.item())\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         Y_hat = model(y0_torch, t)\n",
        "#         final_rmse = rmse(Y_hat, Y_true_t)\n",
        "\n",
        "#     results[name] = {\n",
        "#         \"loss_hist\": loss_hist,\n",
        "#         \"final_loss\": loss_hist[-1],\n",
        "#         \"final_rmse\": final_rmse,\n",
        "#     }\n",
        "\n",
        "#     print(f\"{name:8s}  final_loss={loss_hist[-1]:.4e}  final_RMSE={final_rmse:.4e}\")\n",
        "\n",
        "# # Plot loss curves\n",
        "# plt.figure()\n",
        "# for name in results:\n",
        "#     plt.plot(results[name][\"loss_hist\"], label=name)\n",
        "# plt.yscale(\"log\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Train MSE Loss\")\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjQy4tkvR1QE",
        "outputId": "b59ceddd-2186-46dc-82ce-09ea5d688d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "euler     final_loss=2.2097e+01  final_RMSE=4.7007e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G6QReAphmz2"
      },
      "outputs": [],
      "source": [
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "f.eval()\n",
        "with torch.no_grad():\n",
        "    Y_hat_full = odeint(f, y0_torch, t, method='rk4').numpy()\n",
        "\n",
        "fig, axs = plt.subplots(4, 1, figsize=(9, 9), sharex=True)\n",
        "names = [r\"$\\theta_1$\", r\"$\\omega_1$\", r\"$\\theta_2$\", r\"$\\omega_2$\"]\n",
        "\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.plot(t_eval, Y_true[:, i], label=\"true\", linewidth=1.5)\n",
        "    ax.plot(t_eval, Y_hat_full[:, i], label=\"model\", alpha=0.8)\n",
        "    ax.set_ylabel(names[i])\n",
        "\n",
        "axs[-1].set_xlabel(\"time (s)\")\n",
        "axs[0].legend()\n",
        "plt.show()\n",
        "\n",
        "# root mean squared error per state\n",
        "rmse = np.sqrt(((Y_hat_full - Y_true)**2).mean(axis=0))\n",
        "print(\"RMSE [θ1, ω1, θ2, ω2] =\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBY9o83zHQ6p"
      },
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "\n",
        "# Load Dataset (probably using Pandas)\n",
        "\n",
        "# Make a train-test split\n",
        "\n",
        "# Train the model with x epochs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}